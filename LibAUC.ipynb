{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install libauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libauc.losses import AUCMLoss, CrossEntropyLoss\n",
    "from libauc.optimizers import PESG, Adam\n",
    "from libauc.models import DenseNet121, DenseNet169\n",
    "from libauc.datasets import CheXpert\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, '..', 'data')\n",
    "CheXpert_train_hidden_features_all = np.load(os.path.join(DATA_DIR,'CheXpert_train_hidden_features_all.npy'))\n",
    "CheXpert_train_labels_all = np.load(os.path.join(DATA_DIR,'CheXpert_train_labels_all.npy'))\n",
    "CheXpert_valid_hidden_features_all = np.load(os.path.join(DATA_DIR,'CheXpert_valid_hidden_features_all.npy'))\n",
    "CheXpert_valid_labels_all = np.load(os.path.join(DATA_DIR,'CheXpert_valid_labels_all.npy'))\n",
    "extra_valid_age_sex_df = pd.read_csv(os.path.join(DATA_DIR,'extra_valid_age_sex.csv'))\n",
    "extra_valid_hidden_features = np.load(os.path.join(DATA_DIR,'extra_valid_hidden_features.npy'))\n",
    "extra_valid_labels = np.load(os.path.join(DATA_DIR,'extra_valid_labels.npy'))\n",
    "extra_valid_images = glob(os.path.join(DATA_DIR, 'extraValid', '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing some sort of label smoothing\n",
    "#U-Ones\n",
    "# Map all -1's to 1's\n",
    "'''\n",
    "print(CheXpert_train_labels_all.shape)\n",
    "CheXpert_train_labels_all[CheXpert_train_labels_all == -1] = 1\n",
    "CheXpert_valid_labels_all[CheXpert_valid_labels_all == -1] = 1\n",
    "'''\n",
    "\n",
    "#U-Ones-LSR\n",
    "for i in range(CheXpert_train_labels_all.shape[0]):\n",
    "    for j in range(CheXpert_train_labels_all.shape[1]):\n",
    "        if CheXpert_train_labels_all[i,j] == -1:\n",
    "            CheXpert_train_labels_all[i,j] = random.uniform(.55, .85)\n",
    "for i in range(CheXpert_valid_labels_all.shape[0]):\n",
    "    for j in range(CheXpert_valid_labels_all.shape[1]):\n",
    "        if CheXpert_valid_labels_all[i,j] == -1:\n",
    "            CheXpert_valid_labels_all[i,j] = random.uniform(.55, .85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData(Dataset):\n",
    "    def __init__(self, X, y, scale_X=True):\n",
    "        if not torch.is_tensor(X):\n",
    "            if scale_X:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X)\n",
    "        if not torch.is_tensor(y):\n",
    "            self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelClassification(nn.Module):\n",
    "    def __init__(self, num_feature, num_class):\n",
    "        super(MultiLabelClassification, self).__init__()\n",
    "        \n",
    "        # create separate classifiers for our outputs\n",
    "        self.classpred = nn.Sequential(\n",
    "            nn.Linear(num_feature, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(64, num_class) \n",
    "            )\n",
    "        \n",
    "        self.classpred2 = nn.Sequential(\n",
    "            nn.Linear(num_feature, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 192),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(192, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_class)\n",
    "            )\n",
    "        \n",
    "        self.classifier = nn.Linear(num_feature, num_class)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.classifier(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = PrepareData(CheXpert_train_hidden_features_all, y=CheXpert_train_labels_all, scale_X=True)\n",
    "testset = PrepareData(CheXpert_valid_hidden_features_all, y=CheXpert_valid_labels_all, scale_X=True)\n",
    "trainloader = DataLoader(trainset,\n",
    "                         batch_size=32,\n",
    "                         shuffle=True,\n",
    "                         num_workers=2)\n",
    "testloader = DataLoader(testset,\n",
    "                        batch_size=32,\n",
    "                        shuffle=False,\n",
    "                        num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramaters\n",
    "SEED = 123\n",
    "BATCH_SIZE = 32\n",
    "imratio = 0.3424\n",
    "lr = 0.05 # using smaller learning rate is better\n",
    "gamma = 500\n",
    "weight_decay = 1e-5\n",
    "margin = 1.0\n",
    "\n",
    "# model\n",
    "model = MultiLabelClassification(num_feature=1024, num_class=5)\n",
    "model = model.cuda()\n",
    "\n",
    "# define loss & optimizer\n",
    "Loss = AUCMLoss(imratio=imratio)\n",
    "optimizer = PESG(model, \n",
    "                 a=Loss.a, \n",
    "                 b=Loss.b, \n",
    "                 alpha=Loss.alpha, \n",
    "                 imratio=imratio, \n",
    "                 lr=lr, \n",
    "                 gamma=gamma, \n",
    "                 margin=margin, \n",
    "                 weight_decay=weight_decay)\n",
    "\n",
    "best_val_auc = 0\n",
    "for epoch in range(4):\n",
    "    if epoch > 0:\n",
    "         optimizer.update_regularizer(decay_factor=10)\n",
    "    for idx, data in enumerate(trainloader):\n",
    "        train_data, train_labels = data\n",
    "        train_data, train_labels = train_data.cuda(), train_labels.cuda()\n",
    "        y_pred = model(train_data)\n",
    "        loss = Loss(y_pred, train_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validation\n",
    "        if idx % 400 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():    \n",
    "                test_pred = []\n",
    "                test_true = [] \n",
    "                for jdx, data in enumerate(testloader):\n",
    "                    test_data, test_label = data\n",
    "                    test_data = test_data.cuda()\n",
    "                    y_pred = model(test_data)\n",
    "                    test_pred.append(y_pred.cpu().detach().numpy())\n",
    "                    test_true.append(test_label.numpy())\n",
    "\n",
    "                test_true = np.concatenate(test_true)\n",
    "                test_pred = np.concatenate(test_pred)\n",
    "                #print(test_true[:100], test_pred[:100])\n",
    "                val_auc =  roc_auc_score(test_true, test_pred) \n",
    "                model.train()\n",
    "\n",
    "                if best_val_auc < val_auc:\n",
    "                    best_val_auc = val_auc\n",
    "\n",
    "            print('Epoch=%s, BatchID=%s, Val_AUC=%.4f, lr=%.4f'%(epoch, idx, val_auc,  optimizer.lr))\n",
    "\n",
    "print ('Best Val_AUC is %.4f'%best_val_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
