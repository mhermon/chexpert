{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install libauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libauc.losses import AUCMLoss, CrossEntropyLoss\n",
    "from libauc.optimizers import PESG, Adam\n",
    "from libauc.models import DenseNet121, DenseNet169\n",
    "from libauc.datasets import CheXpert\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from datasets import CheXpertDataset\n",
    "from model import MultiLabelClassification\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(SEED):\n",
    "    # REPRODUCIBILITY\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "set_all_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, '..', 'data')\n",
    "CheXpert_train_hidden_features_all = np.load(os.path.join(DATA_DIR,'CheXpert_train_hidden_features_all.npy'))\n",
    "CheXpert_train_labels_all = np.load(os.path.join(DATA_DIR,'CheXpert_train_labels_all.npy'))\n",
    "CheXpert_valid_hidden_features_all = np.load(os.path.join(DATA_DIR,'CheXpert_valid_hidden_features_all.npy'))\n",
    "CheXpert_valid_labels_all = np.load(os.path.join(DATA_DIR,'CheXpert_valid_labels_all.npy'))\n",
    "extra_valid_age_sex_df = pd.read_csv(os.path.join(DATA_DIR,'extra_valid_age_sex.csv'))\n",
    "extra_valid_hidden_features = np.load(os.path.join(DATA_DIR,'extra_valid_hidden_features.npy'))\n",
    "extra_valid_labels = np.load(os.path.join(DATA_DIR,'extra_valid_labels.npy'))\n",
    "extra_valid_images = glob(os.path.join(DATA_DIR, 'extraValid', '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_labels(method, labels):\n",
    "    if method == \"ones\":\n",
    "        labels[labels == -1] = 1\n",
    "    elif method == \"ones-lsr\":\n",
    "        for i in range(labels.shape[0]):\n",
    "            for j in range(labels.shape[1]):\n",
    "                if labels[i,j] == -1:\n",
    "                    labels[i,j] = random.uniform(.55, .85)\n",
    "    elif method == \"zeros\":\n",
    "        labels[labels == -1] = 0\n",
    "        labels[labels == -1] = 0\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, num_features, num_classes):\n",
    "    \n",
    "    imratio = 0.3424\n",
    "    lr = 0.05 # using smaller learning rate is better\n",
    "    gamma = 500\n",
    "    weight_decay = 1e-5\n",
    "    margin = 1.0\n",
    "\n",
    "    # model\n",
    "    model = MultiLabelClassification(num_feature=num_features, num_class=num_classes)\n",
    "    model = model.cuda()\n",
    "    \n",
    "    # define loss & optimizer\n",
    "    Loss = AUCMLoss(imratio=imratio)\n",
    "    optimizer = PESG(model, \n",
    "                     a=Loss.a, \n",
    "                     b=Loss.b, \n",
    "                     alpha=Loss.alpha, \n",
    "                     imratio=imratio, \n",
    "                     lr=lr, \n",
    "                     gamma=gamma, \n",
    "                     margin=margin, \n",
    "                     weight_decay=weight_decay)\n",
    "\n",
    "    best_val_auc = 0\n",
    "    for epoch in range(2):\n",
    "        if epoch > 0:\n",
    "             optimizer.update_regularizer(decay_factor=10)\n",
    "        for idx, data in enumerate(train_loader):\n",
    "            train_data, train_labels = data\n",
    "            train_data, train_labels = train_data.cuda(), train_labels.cuda()\n",
    "            y_pred = model(train_data)\n",
    "            loss = Loss(y_pred, train_labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # validation\n",
    "            if idx % 400 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():    \n",
    "                    test_pred = []\n",
    "                    test_true = [] \n",
    "                    for jdx, data in enumerate(val_loader):\n",
    "                        test_data, test_label = data\n",
    "                        test_data = test_data.cuda()\n",
    "                        y_pred = model(test_data)\n",
    "                        test_pred.append(y_pred.cpu().detach().numpy())\n",
    "                        test_true.append(test_label.numpy())\n",
    "\n",
    "                    test_true = np.concatenate(test_true)\n",
    "                    test_pred = np.concatenate(test_pred)\n",
    "                    val_auc =  roc_auc_score(test_true, test_pred) \n",
    "                    model.train()\n",
    "\n",
    "                    if best_val_auc < val_auc:\n",
    "                        best_val_auc = val_auc\n",
    "\n",
    "    print ('Best Val_AUC is %.4f'%best_val_auc)\n",
    "    return best_val_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheXpert_X = np.concatenate((CheXpert_train_hidden_features_all, CheXpert_valid_hidden_features_all), axis=0)\n",
    "CheXpert_y = np.concatenate((CheXpert_train_labels_all, CheXpert_valid_labels_all), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing some sort of label smoothing\n",
    "CheXpert_y = smooth_labels(\"ones\", CheXpert_y)\n",
    "extra_valid_labels = smooth_labels(\"ones\", extra_valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf1 = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "extra_train_indices = []\n",
    "extra_val_indices = []\n",
    "for train_index, val_index in kf1.split(extra_valid_hidden_features):\n",
    "    extra_train_indices.append(train_index)\n",
    "    extra_val_indices.append(val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf2 = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "idx = 0\n",
    "aucs = []\n",
    "for train_index, val_index in kf2.split(CheXpert_X):\n",
    "    \n",
    "    # Get CheXpert train and test datasets\n",
    "    X_train_CheXpert, X_val_CheXpert = CheXpert_X[train_index], CheXpert_X[val_index]\n",
    "    y_train_CheXpert, y_val_CheXpert = CheXpert_y[train_index], CheXpert_y[val_index]\n",
    "\n",
    "    # Get hidden train and test datasets\n",
    "    X_train_extra, X_val_extra = extra_valid_hidden_features[extra_train_indices[idx]], extra_valid_hidden_features[extra_val_indices[idx]]\n",
    "    y_train_extra, y_val_extra = extra_valid_labels[extra_train_indices[idx]], extra_valid_labels[extra_val_indices[idx]]\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    '''X_train = np.concatenate((X_train_CheXpert, X_train_extra), axis=0)\n",
    "    np.random.shuffle(X_train)\n",
    "    X_val = np.concatenate((X_val_CheXpert, X_val_extra), axis=0)\n",
    "    np.random.shuffle(X_val)\n",
    "    y_train = np.concatenate((y_train_CheXpert, y_train_extra), axis=0)\n",
    "    np.random.shuffle(y_train)\n",
    "    y_val = np.concatenate((y_val_CheXpert, y_val_extra), axis=0)\n",
    "    np.random.shuffle(y_val)'''\n",
    "    \n",
    "    # Load into dataloaders\n",
    "    train_set = CheXpertDataset(X_train_CheXpert, y=y_train_CheXpert, scale_X=True)\n",
    "    val_set = CheXpertDataset(X_val_CheXpert, y=y_val_CheXpert, scale_X=True)\n",
    "    train_loader = DataLoader(train_set,\n",
    "                             batch_size=32,\n",
    "                             shuffle=True)\n",
    "    val_loader = DataLoader(val_set,\n",
    "                            batch_size=32,\n",
    "                            shuffle=False)\n",
    "    \n",
    "    num_features = X_train_CheXpert.shape[1]\n",
    "    num_classes = y_train_CheXpert.shape[1]\n",
    "    \n",
    "    auc = train(train_loader, val_loader, num_features, num_classes)\n",
    "    aucs.append(auc)\n",
    "    # Training...\n",
    "    idx += 1\n",
    "print(f'Average AUC {np.mean(aucs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CheXpertDataset(CheXpert_train_hidden_features_all, y=CheXpert_train_labels_all, scale_X=True)\n",
    "testset = CheXpertDataset(CheXpert_valid_hidden_features_all, y=CheXpert_valid_labels_all, scale_X=True)\n",
    "trainloader = DataLoader(trainset,\n",
    "                         batch_size=32,\n",
    "                         shuffle=True,\n",
    "                         num_workers=2)\n",
    "testloader = DataLoader(testset,\n",
    "                        batch_size=32,\n",
    "                        shuffle=False,\n",
    "                        num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramaters\n",
    "SEED = 123\n",
    "BATCH_SIZE = 32\n",
    "imratio = 0.3424\n",
    "lr = 0.05 # using smaller learning rate is better\n",
    "gamma = 500\n",
    "weight_decay = 1e-5\n",
    "margin = 1.0\n",
    "\n",
    "# model\n",
    "model = MultiLabelClassification(num_feature=1024, num_class=5)\n",
    "model = model.cuda()\n",
    "\n",
    "# define loss & optimizer\n",
    "Loss = AUCMLoss(imratio=imratio)\n",
    "optimizer = PESG(model, \n",
    "                 a=Loss.a, \n",
    "                 b=Loss.b, \n",
    "                 alpha=Loss.alpha, \n",
    "                 imratio=imratio, \n",
    "                 lr=lr, \n",
    "                 gamma=gamma, \n",
    "                 margin=margin, \n",
    "                 weight_decay=weight_decay)\n",
    "\n",
    "best_val_auc = 0\n",
    "for epoch in range(2):\n",
    "    if epoch > 0:\n",
    "         optimizer.update_regularizer(decay_factor=10)\n",
    "    for idx, data in enumerate(trainloader):\n",
    "        train_data, train_labels = data\n",
    "        train_data, train_labels = train_data.cuda(), train_labels.cuda()\n",
    "        y_pred = model(train_data)\n",
    "        loss = Loss(y_pred, train_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validation\n",
    "        if idx % 400 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():    \n",
    "                test_pred = []\n",
    "                test_true = [] \n",
    "                for jdx, data in enumerate(testloader):\n",
    "                    test_data, test_label = data\n",
    "                    test_data = test_data.cuda()\n",
    "                    y_pred = model(test_data)\n",
    "                    test_pred.append(y_pred.cpu().detach().numpy())\n",
    "                    test_true.append(test_label.numpy())\n",
    "\n",
    "                test_true = np.concatenate(test_true)\n",
    "                test_pred = np.concatenate(test_pred)\n",
    "                val_auc =  roc_auc_score(test_true, test_pred) \n",
    "                model.train()\n",
    "\n",
    "                if best_val_auc < val_auc:\n",
    "                    best_val_auc = val_auc\n",
    "\n",
    "            print('Epoch=%s, BatchID=%s, Val_AUC=%.4f, lr=%.4f'%(epoch, idx, val_auc,  optimizer.lr))\n",
    "\n",
    "print ('Best Val_AUC is %.4f'%best_val_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extratestset = ChexpertDataset(extra_valid_hidden_features, y=extra_valid_labels, scale_X=True)\n",
    "extratestloader = DataLoader(extratestset,\n",
    "                         batch_size=32,\n",
    "                         shuffle=True,\n",
    "                         num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():    \n",
    "    test_pred = []\n",
    "    test_true = [] \n",
    "    for jdx, data in enumerate(extratestloader):\n",
    "        test_data, test_label = data\n",
    "        test_data = test_data.cuda()\n",
    "        y_pred = model(test_data)\n",
    "        test_pred.append(y_pred.cpu().detach().numpy())\n",
    "        test_true.append(test_label.numpy())\n",
    "    \n",
    "    \n",
    "    test_true = np.concatenate(test_true)\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "    auc =  roc_auc_score(test_true, test_pred) \n",
    "\n",
    "    print(auc)\n",
    "\n",
    "print ('Test AUC is %.4f'%auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on actual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#U-Ones-LSR\n",
    "'''\n",
    "extra_valid_age_sex_df\n",
    "extra_valid_hidden_features\n",
    "extra_valid_labels\n",
    "extra_valid_images\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_valid_age_sex_df['sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_valid_age_sex_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to numpy array and then switch M to 1 and F to 0\n",
    "extra_valid_age_sex_df = extra_valid_age_sex_df.replace(['F', 'M', 'O'], [0, 1, 2])\n",
    "extra_valid_age_sex_df = extra_valid_age_sex_df.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_valid_age_sex_np = extra_valid_age_sex_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with the numpy array append to to hidden feature vectors\n",
    "print(extra_valid_age_sex_np.shape, extra_valid_hidden_features.shape)\n",
    "hidden_with_age_sex = np.concatenate((extra_valid_hidden_features, extra_valid_age_sex_np), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, and validation split\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
